{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d39722ed",
   "metadata": {},
   "source": [
    "# ğŸš€ Fluxo Completo de MLOps - PreparaÃ§Ã£o para Entrevista Loft\n",
    "\n",
    "Este notebook cobre o **ciclo completo de deploy e monitoramento de modelos em produÃ§Ã£o** - exatamente o diferencial da vaga!\n",
    "\n",
    "## ğŸ“‹ O que vocÃª vai aprender:\n",
    "1. **PrÃ©-processamento** - Preparar dados para ML\n",
    "2. **Treinamento** - Treinar e comparar modelos\n",
    "3. **Tracking com MLflow** - Rastrear experimentos\n",
    "4. **Registry** - Versionar modelos\n",
    "5. **Scoring em Batch** - PrediÃ§Ãµes em lote\n",
    "6. **Pipeline Vertex AI** - AutomaÃ§Ã£o completa\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c493f00",
   "metadata": {},
   "source": [
    "## ğŸ”§ InstalaÃ§Ã£o das DependÃªncias\n",
    "\n",
    "Execute esta cÃ©lula para instalar tudo que precisa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7cf30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy scikit-learn xgboost mlflow google-cloud-storage google-cloud-aiplatform kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302331f4",
   "metadata": {},
   "source": [
    "## ğŸ“Š Parte 1: Entendendo os Dados\n",
    "\n",
    "Primeiro, vamos carregar e explorar os dados. Isso Ã© fundamental para qualquer projeto de ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813981dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# ConfiguraÃ§Ãµes GCP\n",
    "PROJECT_ID = \"mlops-484912\"\n",
    "BUCKET_NAME = \"meu-bucket-29061999\"\n",
    "\n",
    "# Carrega dados (local ou GCS)\n",
    "# OpÃ§Ã£o 1: Local\n",
    "df = pd.read_csv(\"../data/base_clientes_inadimplencia.csv\")\n",
    "\n",
    "# OpÃ§Ã£o 2: Do GCS (descomente se os dados estiverem no bucket)\n",
    "# df = pd.read_csv(f\"gs://{BUCKET_NAME}/data/base_clientes_inadimplencia.csv\")\n",
    "\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColunas: {list(df.columns)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e91be14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnÃ¡lise da variÃ¡vel target\n",
    "print(\"DistribuiÃ§Ã£o do Target (Status_Pagamento):\")\n",
    "print(df[\"Status_Pagamento\"].value_counts())\n",
    "print(f\"\\n% Inadimplentes: {(df['Status_Pagamento'] == 'Inadimplente').mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1849e8",
   "metadata": {},
   "source": [
    "## ğŸ”„ Parte 2: PrÃ©-processamento\n",
    "\n",
    "Esta etapa Ã© CRUCIAL em qualquer projeto de ML. Um bom prÃ©-processamento pode melhorar muito a performance do modelo.\n",
    "\n",
    "**O que fazemos:**\n",
    "- Tratar valores nulos\n",
    "- Criar features (engenharia de features)\n",
    "- Codificar variÃ¡veis categÃ³ricas\n",
    "- Normalizar dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb20bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "def preprocessar_dados(df):\n",
    "    \"\"\"\n",
    "    Pipeline de prÃ©-processamento.\n",
    "    \n",
    "    DICA PARA ENTREVISTA: Explique cada decisÃ£o!\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Tratar valores nulos\n",
    "    # NumÃ©ricos: mediana (robusta a outliers)\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    # CategÃ³ricos: valor mais frequente ou 'desconhecido'\n",
    "    for col in df.select_dtypes(include=[object]).columns:\n",
    "        df[col] = df[col].fillna('desconhecido')\n",
    "    \n",
    "    # 2. Feature Engineering: Calcular idade\n",
    "    if \"Data_Nascimento\" in df.columns:\n",
    "        df[\"Data_Nascimento\"] = pd.to_datetime(df[\"Data_Nascimento\"], errors='coerce')\n",
    "        ref = datetime.today()\n",
    "        df[\"Idade\"] = ref.year - df[\"Data_Nascimento\"].dt.year\n",
    "    \n",
    "    # 3. Remover colunas que nÃ£o ajudam na prediÃ§Ã£o\n",
    "    drop_cols = [\"Telefone\", \"Nome\", \"Email\", \"Data_Nascimento\", \n",
    "                 \"Data_Contratacao\", \"Data_Vencimento_Fatura\", \n",
    "                 \"Data_Ingestao\", \"Data_Atualizacao\"]\n",
    "    df = df.drop(columns=[c for c in drop_cols if c in df.columns], errors='ignore')\n",
    "    \n",
    "    # 4. Codificar target\n",
    "    if \"Status_Pagamento\" in df.columns:\n",
    "        le = LabelEncoder()\n",
    "        df[\"Status_Pagamento\"] = le.fit_transform(df[\"Status_Pagamento\"])\n",
    "    \n",
    "    # 5. One-hot encoding para categÃ³ricas\n",
    "    cat_cols = df.select_dtypes(include=['object']).columns\n",
    "    df = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Aplica prÃ©-processamento\n",
    "df_processed = preprocessar_dados(df)\n",
    "print(f\"Shape apÃ³s processamento: {df_processed.shape}\")\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e273dc",
   "metadata": {},
   "source": [
    "## ğŸ¤– Parte 3: Treinamento com MLflow Tracking\n",
    "\n",
    "**MLflow** Ã© a ferramenta padrÃ£o de mercado para rastrear experimentos de ML.\n",
    "\n",
    "**Por que usar MLflow?**\n",
    "- Rastreia parÃ¢metros, mÃ©tricas e artefatos\n",
    "- Compara diferentes runs/modelos\n",
    "- Versiona modelos\n",
    "- Facilita reprodutibilidade\n",
    "\n",
    "**ISSO Ã‰ O QUE A LOFT VAI PERGUNTAR!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8904ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.xgboost\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Configura MLflow\n",
    "# OpÃ§Ã£o 1: Local (para desenvolvimento)\n",
    "mlflow.set_tracking_uri(\"mlruns\")  # Salva localmente\n",
    "\n",
    "# OpÃ§Ã£o 2: GCS (para produÃ§Ã£o - descomente)\n",
    "# mlflow.set_tracking_uri(f\"gs://{BUCKET_NAME}/mlflow\")\n",
    "\n",
    "# Cria experimento\n",
    "mlflow.set_experiment(\"inadimplencia-experimentos\")\n",
    "\n",
    "print(\"MLflow configurado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a139ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara dados para treino\n",
    "target = \"Status_Pagamento\"\n",
    "X = df_processed.drop(columns=[target, \"ID_Cliente\"], errors='ignore')\n",
    "y = df_processed[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Treino: {X_train.shape}\")\n",
    "print(f\"Teste: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b444950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treinar_e_logar(model, model_name, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Treina modelo e registra tudo no MLflow.\n",
    "    \n",
    "    IMPORTANTE PARA ENTREVISTA:\n",
    "    - Explique o que cada mÃ©trica significa\n",
    "    - Por que escolheu essas mÃ©tricas?\n",
    "    \"\"\"\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        # 1. Log dos parÃ¢metros\n",
    "        mlflow.log_param(\"model_type\", model_name)\n",
    "        mlflow.log_param(\"train_size\", len(X_train))\n",
    "        mlflow.log_param(\"test_size\", len(X_test))\n",
    "        mlflow.log_param(\"n_features\", X_train.shape[1])\n",
    "        \n",
    "        # 2. Treina\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # 3. PrediÃ§Ãµes\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # 4. Calcula mÃ©tricas\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"f1_score\": f1_score(y_test, y_pred),\n",
    "            \"precision\": precision_score(y_test, y_pred),\n",
    "            \"recall\": recall_score(y_test, y_pred),\n",
    "            \"roc_auc\": roc_auc_score(y_test, y_proba),\n",
    "        }\n",
    "        \n",
    "        # 5. Log das mÃ©tricas\n",
    "        for k, v in metrics.items():\n",
    "            mlflow.log_metric(k, v)\n",
    "            print(f\"{k}: {v:.4f}\")\n",
    "        \n",
    "        # 6. Log do modelo\n",
    "        if \"XGB\" in model_name:\n",
    "            mlflow.xgboost.log_model(model, \"model\")\n",
    "        else:\n",
    "            mlflow.sklearn.log_model(model, \"model\")\n",
    "        \n",
    "        # 7. Registra modelo no Model Registry\n",
    "        mlflow.register_model(\n",
    "            f\"runs:/{mlflow.active_run().info.run_id}/model\",\n",
    "            model_name\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nâœ… Modelo {model_name} registrado com sucesso!\")\n",
    "        return model, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53746a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treina XGBoost\n",
    "print(\"=\"*50)\n",
    "print(\"TREINANDO XGBOOST\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_model, xgb_metrics = treinar_e_logar(\n",
    "    xgb_model, \"XGBoost-Inadimplencia\", \n",
    "    X_train, X_test, y_train, y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d65829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treina Random Forest\n",
    "print(\"=\"*50)\n",
    "print(\"TREINANDO RANDOM FOREST\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model, rf_metrics = treinar_e_logar(\n",
    "    rf_model, \"RandomForest-Inadimplencia\",\n",
    "    X_train, X_test, y_train, y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb19d5ee",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Parte 4: Comparando Modelos\n",
    "\n",
    "Uma das perguntas clÃ¡ssicas de entrevista: **\"Como vocÃª escolhe o melhor modelo?\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed31d6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compara mÃ©tricas\n",
    "comparison = pd.DataFrame({\n",
    "    \"XGBoost\": xgb_metrics,\n",
    "    \"Random Forest\": rf_metrics\n",
    "}).T\n",
    "\n",
    "print(\"\\nğŸ“Š COMPARAÃ‡ÃƒO DOS MODELOS:\")\n",
    "print(\"=\"*50)\n",
    "print(comparison.round(4))\n",
    "\n",
    "# Qual Ã© o melhor?\n",
    "best_model = \"XGBoost\" if xgb_metrics[\"f1_score\"] > rf_metrics[\"f1_score\"] else \"Random Forest\"\n",
    "print(f\"\\nğŸ† Melhor modelo (por F1-Score): {best_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615a5069",
   "metadata": {},
   "source": [
    "## ğŸš€ Parte 5: Scoring em Batch (ProduÃ§Ã£o)\n",
    "\n",
    "Agora vamos simular como funciona em **produÃ§Ã£o**: carregar modelo do registry e fazer prediÃ§Ãµes em novos dados.\n",
    "\n",
    "**Este Ã© o deploy de modelo!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e89296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring_batch(model_name, model_version, dados_novos):\n",
    "    \"\"\"\n",
    "    Faz scoring em batch - padrÃ£o de produÃ§Ã£o.\n",
    "    \n",
    "    Em produÃ§Ã£o real:\n",
    "    1. Dados vÃªm de um bucket (GCS/S3)\n",
    "    2. Modelo Ã© carregado do registry\n",
    "    3. PrediÃ§Ãµes sÃ£o salvas no bucket\n",
    "    4. Alertas sÃ£o disparados se necessÃ¡rio\n",
    "    \"\"\"\n",
    "    # Carrega modelo do registry\n",
    "    model_uri = f\"models:/{model_name}/{model_version}\"\n",
    "    print(f\"Carregando modelo: {model_uri}\")\n",
    "    \n",
    "    model = mlflow.pyfunc.load_model(model_uri)\n",
    "    \n",
    "    # Faz prediÃ§Ãµes\n",
    "    predictions = model.predict(dados_novos)\n",
    "    \n",
    "    # Monta resultado\n",
    "    resultado = dados_novos.copy()\n",
    "    resultado[\"prediction\"] = predictions\n",
    "    \n",
    "    return resultado\n",
    "\n",
    "# Simula novos dados (usa o teste como exemplo)\n",
    "print(\"\\nğŸ”® EXECUTANDO SCORING EM BATCH\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "resultado = scoring_batch(\"XGBoost-Inadimplencia\", \"1\", X_test)\n",
    "print(f\"\\nPrediÃ§Ãµes geradas: {len(resultado)}\")\n",
    "print(f\"Inadimplentes previstos: {resultado['prediction'].sum()}\")\n",
    "resultado.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c1f883",
   "metadata": {},
   "source": [
    "## â˜ï¸ Parte 6: Pipeline no Vertex AI (ProduÃ§Ã£o GCP)\n",
    "\n",
    "**IMPORTANTE:** Esta parte precisa rodar em uma mÃ¡quina com acesso ao GCP!\n",
    "\n",
    "VocÃª pode:\n",
    "1. Usar o **Cloud Shell** do GCP (gratuito)\n",
    "2. Usar **Vertex AI Workbench** (notebook gerenciado)\n",
    "3. Configurar gcloud na sua mÃ¡quina local\n",
    "\n",
    "O cÃ³digo abaixo mostra como criar e executar um pipeline completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7bd4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CÃ“DIGO PARA RODAR NO GCP\n",
    "# ==========================\n",
    "# Copie este cÃ³digo para o Cloud Shell ou Vertex AI Workbench\n",
    "\n",
    "PIPELINE_CODE = '''\n",
    "from google.cloud import aiplatform\n",
    "from kfp import dsl, compiler\n",
    "from kfp.dsl import component, Input, Output, Dataset, Model, Metrics\n",
    "\n",
    "PROJECT_ID = \"mlops-484912\"\n",
    "REGION = \"us-central1\"\n",
    "BUCKET_NAME = \"meu-bucket-29061999\"\n",
    "\n",
    "# Inicializa Vertex AI\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=f\"gs://{BUCKET_NAME}\"\n",
    ")\n",
    "\n",
    "@component(base_image=\"python:3.10-slim\", packages_to_install=[\"pandas\", \"scikit-learn\", \"xgboost\"])\n",
    "def treinar_modelo(input_path: str, output_model: Output[Model], output_metrics: Output[Metrics]):\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from xgboost import XGBClassifier\n",
    "    import pickle\n",
    "    \n",
    "    df = pd.read_csv(input_path)\n",
    "    # ... (preprocessamento e treino)\n",
    "    model = XGBClassifier()\n",
    "    # model.fit(...)\n",
    "    \n",
    "    with open(output_model.path, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "@dsl.pipeline(name=\"pipeline-inadimplencia\")\n",
    "def ml_pipeline(input_data: str):\n",
    "    train_task = treinar_modelo(input_path=input_data)\n",
    "\n",
    "# Compila e executa\n",
    "compiler.Compiler().compile(ml_pipeline, \"pipeline.json\")\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"inadimplencia-run\",\n",
    "    template_path=\"pipeline.json\",\n",
    "    pipeline_root=f\"gs://{BUCKET_NAME}/pipeline_root\",\n",
    ")\n",
    "job.submit()\n",
    "'''\n",
    "\n",
    "print(\"ğŸ“‹ CÃ³digo do Pipeline Vertex AI:\")\n",
    "print(\"=\"*50)\n",
    "print(\"Copie o cÃ³digo de jobs/vertex_pipeline.py e execute no GCP!\")\n",
    "print(\"\\nOpÃ§Ãµes para executar:\")\n",
    "print(\"1. Cloud Shell: https://shell.cloud.google.com\")\n",
    "print(\"2. Vertex AI Workbench: https://console.cloud.google.com/vertex-ai/workbench\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4f73f7",
   "metadata": {},
   "source": [
    "## ğŸ“š Resumo para Entrevista\n",
    "\n",
    "### Ciclo Completo de MLOps:\n",
    "\n",
    "```\n",
    "1. DADOS â†’ 2. PREPROCESSAMENTO â†’ 3. TREINAMENTO â†’ 4. AVALIAÃ‡ÃƒO â†’ 5. REGISTRO â†’ 6. DEPLOY â†’ 7. MONITORAMENTO\n",
    "     â†‘                                                                                              |\n",
    "     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ FEEDBACK / RETREINO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Ferramentas que vocÃª usou:\n",
    "- **MLflow**: Tracking de experimentos e Model Registry\n",
    "- **Google Cloud Storage**: Armazenamento de dados e modelos\n",
    "- **Vertex AI Pipelines**: OrquestraÃ§Ã£o e automaÃ§Ã£o\n",
    "- **Kubeflow Pipelines (KFP)**: DefiniÃ§Ã£o de pipelines\n",
    "\n",
    "### Perguntas comuns de entrevista:\n",
    "\n",
    "1. **\"Como vocÃª versiona modelos?\"**\n",
    "   - Resposta: MLflow Model Registry com tags de versÃ£o e stages (Staging, Production)\n",
    "\n",
    "2. **\"Como vocÃª monitora modelos em produÃ§Ã£o?\"**\n",
    "   - Resposta: MÃ©tricas de performance, data drift detection, logs estruturados\n",
    "\n",
    "3. **\"Como vocÃª faz deploy de um modelo?\"**\n",
    "   - Resposta: Batch scoring via pipeline ou endpoint REST para real-time\n",
    "\n",
    "4. **\"Como vocÃª garante reprodutibilidade?\"**\n",
    "   - Resposta: Pipelines versionados, containers Docker, tracking de experimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b6ed9b",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ¯ PrÃ³ximos Passos\n",
    "\n",
    "1. **Upload dos dados para o GCS:**\n",
    "   ```bash\n",
    "   gsutil cp data/base_clientes_inadimplencia.csv gs://meu-bucket-29061999/data/\n",
    "   ```\n",
    "\n",
    "2. **Executar o pipeline no Vertex AI:**\n",
    "   ```bash\n",
    "   python jobs/vertex_pipeline.py\n",
    "   ```\n",
    "\n",
    "3. **Visualizar no console:**\n",
    "   - https://console.cloud.google.com/vertex-ai/pipelines\n",
    "\n",
    "Boa sorte na entrevista! ğŸš€"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
